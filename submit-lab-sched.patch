diff --git a/kernel/proc.c b/kernel/proc.c
index 959b778..6d0b9ad 100644
--- a/kernel/proc.c
+++ b/kernel/proc.c
@@ -9,6 +9,12 @@
 struct cpu cpus[NCPU];
 
 struct proc proc[NPROC];
+struct list_head runq;
+
+struct data_holder {
+  struct list_head list;
+  struct proc p;
+};
 
 struct proc *initproc;
 
@@ -26,9 +32,16 @@ extern char trampoline[]; // trampoline.S
 // must be acquired before any p->lock.
 struct spinlock wait_lock;
 
+static inline void
+__wfi(void)
+{
+  asm volatile("wfi");
+}
+
 // Allocate a page for each process's kernel stack.
 // Map it high in memory, followed by an invalid
 // guard page.
+
 void
 proc_mapstacks(pagetable_t kpgtbl)
 {
@@ -441,16 +454,22 @@ wait(uint64 addr)
 //  - swtch to start running that process.
 //  - eventually that process transfers control
 //    via swtch back to the scheduler.
+
 void
 scheduler(void)
 {
   struct proc *p;
   struct cpu *c = mycpu();
-  
+  int not_runnable_count = 0;
+
+  // create the queue
+  init_list_head(&runq);
+
   c->proc = 0;
   for(;;){
     // Avoid deadlock by ensuring that devices can interrupt.
     intr_on();
+    not_runnable_count = 0;
 
     for(p = proc; p < &proc[NPROC]; p++) {
       acquire(&p->lock);
@@ -465,9 +484,14 @@ scheduler(void)
         // Process is done running for now.
         // It should have changed its p->state before coming back.
         c->proc = 0;
+      } else if (p->state != RUNNABLE) {
+        not_runnable_count++;
       }
       release(&p->lock);
     }
+    if (not_runnable_count == NPROC) {
+      __wfi();
+    }
   }
 }
 
